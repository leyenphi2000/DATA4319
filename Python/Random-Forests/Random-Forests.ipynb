{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5630ded",
   "metadata": {},
   "source": [
    "# Project - Random Forests\n",
    "## Phi Le\n",
    "### DATA - 4319\n",
    "### FALL 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03153859",
   "metadata": {},
   "source": [
    "Random Forests overcomes the overfitting problem of decision trees and helps in solving both classification and regression problems. It works on the principle of $\\textbf{Ensemble learning}$.\n",
    "\n",
    "The Ensemble learning methods believe that a large number of weak learners can work together for giving $\\textbf{high accuracy predictions}$.\n",
    "\n",
    "Random Forests work in a much similar way. It considers the prediction of a large number of individual decision trees for giving the final outcome. It calculates the number of votes of predictions of different decision trees and the prediction with the largest number of votes becomes the prediction of the model.\n",
    "\n",
    "Let us understand this by an example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e19b5",
   "metadata": {},
   "source": [
    "![](https://i2.wp.com/techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/03/random-forests-algorithm.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c01d5",
   "metadata": {},
   "source": [
    "In the above image, there are two classes labeled as A and B. In this random forest consisting of 7 decision trees, 3 have voted for class A and 4 voted for class B. As class B has received the maximum votes thus the modelâ€™s prediction will be class B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306006f",
   "metadata": {},
   "source": [
    "**Random forests (RF)** construct many individual decision trees at training. Predictions from all trees are pooled to make the final prediction; the mode of the classes for classification or the mean prediction for regression. As they use a collection of results to make a final decision, they are referred to as Ensemble techniques.\n",
    "\n",
    "**Feature Importance**\n",
    "\n",
    "Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.\n",
    "\n",
    "**Implementation in Scikit-learn**\n",
    "\n",
    "For each decision tree, Scikit-learn calculates a nodes importance using Gini Importance, assuming only two child nodes (binary tree):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cad6c",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*C-bkgMBs4drNVyBb1VJcEQ.png)\n",
    "\n",
    "- $ni_{(j)}$ = the importance of node j\n",
    "- $w_{(j)}$ = weighted number of samples reaching node j\n",
    "- $C_{(j)}$= the impurity value of node j\n",
    "- $left(j)$ = child node from left split on node j\n",
    "- $right(j)$ = child node from right split on node j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881462b",
   "metadata": {},
   "source": [
    "The importance for each feature on a decision tree is then calculated as:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*oar13be_cUsLR35MA_t6WQ.png)\n",
    "\n",
    "- $fi_(i)$ = the importance of feature i\n",
    "- $ni_(j)$ = the importance of node j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f23f7",
   "metadata": {},
   "source": [
    "### Project Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baaa5a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a1ffbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c0294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.903 (0.028)\n"
     ]
    }
   ],
   "source": [
    "# evaluate random forest algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0657a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# make predictions using random forest for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[-8.52381793,5.24451077,-12.14967704,-2.92949242,0.99314133,0.67326595,-0.38657932,1.27955683,-0.60712621,3.20807316,0.60504151,-1.38706415,8.92444588,-7.43027595,-2.33653219,1.10358169,0.21547782,1.05057966,0.6975331,0.26076035]]\n",
    "yhat = model.predict(row)\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699224c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# test regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=2)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca48063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -90.100 (8.170)\n"
     ]
    }
   ],
   "source": [
    "# evaluate random forest ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=2)\n",
    "# define the model\n",
    "model = RandomForestRegressor()\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac193849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: -161\n"
     ]
    }
   ],
   "source": [
    "# random forest for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=2)\n",
    "# define the model\n",
    "model = RandomForestRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[-0.89483109,-1.0670149,-0.25448694,-0.53850126,0.21082105,\n",
    "        1.37435592,0.71203659,0.73093031,-1.25878104,-2.01656886,\n",
    "        0.51906798,0.62767387,0.96250155,1.31410617,-1.25527295,\n",
    "        -0.85079036,0.24129757,-0.17571721,-1.11454339,0.36268268]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fae6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
