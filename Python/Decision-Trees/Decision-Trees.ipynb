{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f16b5c",
   "metadata": {},
   "source": [
    "# Project - Decision Trees\n",
    "## Phi Le\n",
    "### DATA - 4319"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e1855",
   "metadata": {},
   "source": [
    "Decision trees help in solving both classification and prediction problems. It makes it easy to understand the data for better accuracy of the predictions. Each node of the Decision tree represents a feature or an attribute, each link represents a decision and each leaf node holds a class label, that is, the outcome.\n",
    "\n",
    "The drawback of decision trees is that it suffers from the problem of overfitting.\n",
    "\n",
    "Basically, these two Data Science algorithms are most commonly used for implementing the Decision trees.\n",
    "\n",
    "- ID3 ( Iterative Dichotomiser 3) Algorithm\n",
    "\n",
    "This algorithm uses entropy and information gain as the decision metric.\n",
    "\n",
    "- Cart ( Classification and Regression Tree) Algorithm\n",
    "\n",
    "This algorithm uses the Gini index as the decision metric. The below image will help you to understand things better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9d000",
   "metadata": {},
   "source": [
    "![](https://i1.wp.com/techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/03/cart-algorithm.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359f9cc",
   "metadata": {},
   "source": [
    "- *Decision Trees* are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves. The leaves are the decisions or the final outcomes. And the decision nodes are where the data is split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bed329",
   "metadata": {},
   "source": [
    "**Entropy**, also called as Shannon Entropy is denoted by H(S) for a finite set S, is the measure of the amount of uncertainty or randomness in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0530f4",
   "metadata": {},
   "source": [
    "![](https://www.xoriant.com/sites/default/files/uploads/2017/08/Decision-Trees-modified-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59457d1c",
   "metadata": {},
   "source": [
    "Intuitively, it tells us about the predictability of a certain event. Example, consider a coin toss whose probability of heads is 0.5 and probability of tails is 0.5. Here the entropy is the highest possible, since there’s no way of determining what the outcome might be. Alternatively, consider a coin which has heads on both the sides, the entropy of such an event can be predicted perfectly since we know beforehand that it’ll always be heads. In other words, this event has no randomness hence it’s entropy is zero. In particular, lower values imply less uncertainty while higher values imply high uncertainty. Information Gain Information gain is also called as Kullback-Leibler divergence denoted by IG(S,A) for a set S is the effective change in entropy after deciding on a particular attribute A. It measures the relative change in entropy with respect to the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21338ab4",
   "metadata": {},
   "source": [
    "$$\n",
    "IG(S,A) = H(S) - H(S,A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fa472",
   "metadata": {},
   "source": [
    "Alternatively,\n",
    "$$\n",
    "IG(S,A) = H(S) - \\sum_{i=0}^n P(x) * H(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f654c6a",
   "metadata": {},
   "source": [
    "where IG(S, A) is the information gain by applying feature A. H(S) is the Entropy of the entire set, while the second term calculates the Entropy after applying the feature A, where P(x) is the probability of event x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7634740",
   "metadata": {},
   "source": [
    "### Project-Example for Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be13f4b",
   "metadata": {},
   "source": [
    "#### Information about the dataset\n",
    "\n",
    "The dataset can be downloaded from here (https://www.kaggle.com/gangliu/drugsets). The data depicts a list of patients with there relevant informations and drug taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ebcab",
   "metadata": {},
   "source": [
    "$\\textbf{Step 1: Import packages and dataset}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce98579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Sex      BP Cholesterol  Na_to_K   Drug\n",
      "0     23   F    HIGH        HIGH   25.355  drugY\n",
      "1     47   M     LOW        HIGH   13.093  drugC\n",
      "2     47   M     LOW        HIGH   10.114  drugC\n",
      "3     28   F  NORMAL        HIGH    7.798  drugX\n",
      "4     61   F     LOW        HIGH   18.043  drugY\n",
      "..   ...  ..     ...         ...      ...    ...\n",
      "195   56   F     LOW        HIGH   11.567  drugC\n",
      "196   16   M     LOW        HIGH   12.006  drugC\n",
      "197   52   M  NORMAL        HIGH    9.894  drugX\n",
      "198   23   M  NORMAL      NORMAL   14.020  drugX\n",
      "199   40   F     LOW      NORMAL   11.349  drugX\n",
      "\n",
      "[200 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datainput = pd.read_csv(\"drug200.csv\", delimiter=\",\") \n",
    "print(datainput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf5f99",
   "metadata": {},
   "source": [
    "$\\textbf{Step 2: Pre-processing the data}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d93f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datainput[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "label_gender = preprocessing.LabelEncoder()\n",
    "label_gender.fit(['F', 'M'])\n",
    "X[:, 1] = label_gender.transform(X[:, 1])\n",
    "\n",
    "label_BP = preprocessing.LabelEncoder()\n",
    "label_BP.fit(['LOW', 'NORMAL', 'HIGH'])\n",
    "X[:, 2] = label_BP.transform(X[:, 2])\n",
    "\n",
    "label_Chol = preprocessing.LabelEncoder()\n",
    "label_Chol.fit(['NORMAL', 'HIGH'])\n",
    "X[:, 3] = label_Chol.transform(X[:, 3])\n",
    "\n",
    "y = datainput[\"Drug\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b01ff",
   "metadata": {},
   "source": [
    "$\\textbf{Step 3: Training the Dataset}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b924d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n",
    "\n",
    "drugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f72ec",
   "metadata": {},
   "source": [
    "$\\textbf{Step 4: Getting the result from Trained Data set}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5adc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drugY' 'drugX' 'drugX' 'drugX' 'drugX' 'drugC' 'drugY' 'drugA' 'drugB'\n",
      " 'drugA' 'drugY' 'drugA' 'drugY' 'drugY' 'drugX' 'drugY' 'drugX' 'drugX'\n",
      " 'drugB' 'drugX' 'drugX' 'drugY' 'drugY' 'drugY' 'drugX' 'drugB' 'drugY'\n",
      " 'drugY' 'drugA' 'drugX' 'drugB' 'drugC' 'drugC' 'drugX' 'drugX' 'drugC'\n",
      " 'drugY' 'drugX' 'drugX' 'drugX' 'drugA' 'drugY' 'drugC' 'drugY' 'drugA'\n",
      " 'drugY' 'drugY' 'drugY' 'drugY' 'drugY' 'drugB' 'drugX' 'drugY' 'drugX'\n",
      " 'drugY' 'drugY' 'drugA' 'drugX' 'drugY' 'drugX']\n",
      "\n",
      "DecisionTrees's Accuracy:  0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "drugTree.fit(X_train, y_train)\n",
    "predicted = drugTree.predict(X_test)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "print(\"\\nDecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b0f0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  0,  0,  0,  0],\n",
       "       [ 0,  5,  0,  0,  0],\n",
       "       [ 0,  0,  5,  0,  0],\n",
       "       [ 0,  0,  0, 20,  1],\n",
       "       [ 0,  0,  0,  0, 22]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the confusion matrix\n",
    "confusion_matrix(y_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
